{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EvXt9p4ufUd"
      },
      "source": [
        "<b>\n",
        "<p>\n",
        "<center>\n",
        "<font size=\"9\">\n",
        "Weather Image Recognition\n",
        "</font>\n",
        "</center>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<font size=\"5\">\n",
        "using\n",
        "</font>\n",
        "</center>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<font size=\"6\">\n",
        " Convolutional Neural Networks(CNNs)\n",
        "</font>\n",
        "</center>\n",
        "</p>\n",
        "    \n",
        "<p>\n",
        "<center>\n",
        "<font size=\"4\">\n",
        "Machine Learning I\n",
        "</font>\n",
        "</center>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<font size=\"3\">\n",
        "Data Science, Columbian College of Arts & Sciences, George Washington University\n",
        "</font>\n",
        "</center>\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "<center>\n",
        "<font size=\"4\">\n",
        "TEAM:\n",
        "</font>\n",
        "<p></p>\n",
        "<font size=\"5\">\n",
        "Ramana Bhaskar Kosuru(G44367009)\n",
        "</font>\n",
        "<p></p>\n",
        "<font size=\"5\">\n",
        "Chaya Chandana Doddaiggaluru Appajigowda(G)\n",
        "</font>\n",
        "<p></p>\n",
        "<font size=\"5\">\n",
        "Sirisha Ginnu(G)\n",
        "</font>\n",
        "<p></p>\n",
        "</center>\n",
        "</p>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT2SKHw2zlEi"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asZWLrJKzlEj"
      },
      "source": [
        "- This notebook includes code for Convolutional Neural Networks used to classify weather images.\n",
        "- Here we will work on the Kaggle Dataset [weather_image_Recognition](https://www.kaggle.com/datasets/jehanbhathena/weather-dataset).\n",
        "- The goal of this project:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJ-IbZqAgILJ"
      },
      "source": [
        "# Notebook Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3yB94KtgMHu"
      },
      "source": [
        "## Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWmYBTOwgNs-",
        "outputId": "1cc32a82-3ed3-41fd-acc4-d9bbd162610b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Get the absolute path of the current folder\n",
        "abspath_curr = '/content/drive/My Drive/Colab Notebooks/ML_1/Final Project/'\n",
        "\n",
        "# Get the absolute path of the deep utilities folder\n",
        "abspath_util_deep = '/content/drive/My Drive/Colab Notebooks/ML_1/code/utilities/p3_deep_learning/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYZhU1Wqgmqx"
      },
      "source": [
        "## Warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MUl4k83e4ANR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import warnings\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WMODpPfgn2U"
      },
      "source": [
        "## Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DBRVH9SB4ANb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Set matplotlib sizes\n",
        "plt.rc('font', size=20)\n",
        "plt.rc('axes', titlesize=20)\n",
        "plt.rc('axes', labelsize=20)\n",
        "plt.rc('xtick', labelsize=20)\n",
        "plt.rc('ytick', labelsize=20)\n",
        "plt.rc('legend', fontsize=20)\n",
        "plt.rc('figure', titlesize=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-wNDk5nZhhO"
      },
      "source": [
        "## TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LjG43tEnZkfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdaf91b-ca88-43c7-a958-50ce8a36feff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# The magic below allows us to use tensorflow version 2.x\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40FN3UNfO2Z7"
      },
      "source": [
        "## Random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uSADk0hJP71d"
      },
      "outputs": [],
      "source": [
        "# The random seed\n",
        "random_seed = 42\n",
        "\n",
        "# Set random seed in tensorflow\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "# Set random seed in numpy\n",
        "import numpy as np\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eOpQpPu4ANk"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D-Mwc6MczlFD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cabadd-561d-4dda-ff3b-eb73d3064769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/ML_1/code/utilities/p3_deep_learning\n"
          ]
        }
      ],
      "source": [
        "# Change working directory to the absolute path of the deep utilities folder\n",
        "%cd $abspath_util_deep\n",
        "\n",
        "# Import the deep utitilities\n",
        "%run pmlm_utilities_deep.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnJAfR784ANl"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM_WKAmO4ANm"
      },
      "source": [
        "In this project, we will work on the [weather_image_recognition](https://www.kaggle.com/datasets/jehanbhathena/weather-dataset/data) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEjvHlLB8X0z"
      },
      "source": [
        "### Creating the directory for the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "reijpcaf8UgO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Make directory\n",
        "directory = os.path.dirname(abspath_curr + '/dataset/')\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sDdbhok2i7C"
      },
      "source": [
        "### Downloading the data to the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6onKpDeL4ANn",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c63e01-8b11-49da-c712-514c049f8fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6862 files belonging to 11 classes.\n",
            "\n",
            "The length of the dataset is  6862\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the dataset\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=abspath_curr + '/dataset/',\n",
        "    image_size=(224, 224),  # Resize images to 224x224\n",
        "    batch_size=1,  # Load one image at a time to count total images\n",
        "    label_mode='int',  # Labels are returned as integers\n",
        "    shuffle=True  # Shuffle the dataset before splitting\n",
        ")\n",
        "print(\"\\nThe length of the dataset is \", len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvIqSuZUVBb8"
      },
      "source": [
        "## Getting the name of the target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AF5gbpOWVElL"
      },
      "outputs": [],
      "source": [
        "target = 'label'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugddFhUgVGWq"
      },
      "source": [
        "## Getting the info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vsSHR38vjL1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc0afe6-c247-4fac-9ebc-d29a1d32761d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dew', 'fogsmog', 'frost', 'glaze', 'hail', 'lightning', 'rain', 'rainbow', 'rime', 'sandstorm', 'snow']\n"
          ]
        }
      ],
      "source": [
        "# Display class names\n",
        "\n",
        "classes = dataset.class_names\n",
        "\n",
        "print(classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hD0wa74kS_53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e66a7f3-f86b-46be-b556-89af5cc16f8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Get the number of classes\n",
        "n_classes = len(classes)\n",
        "\n",
        "# Print the number of classes\n",
        "len(classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGLoCbmO4AN9"
      },
      "source": [
        "## Getting the training, validation and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a2P4L1vS4AN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94a6795-fd73-4d8e-b6c7-2d8b85af4b0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6862\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Get the total number of images\n",
        "total_size = len(dataset)\n",
        "\n",
        "print(total_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the sizes for Train (70%), Validation (15%), and Test (15%)\n",
        "train_size = int(0.70 * total_size)\n",
        "val_size = int(0.15 * total_size)\n",
        "test_size = total_size - train_size - val_size  # The remaining 15%\n",
        "\n",
        "# Split the dataset using random_split\n",
        "data_train = dataset.take(train_size)\n",
        "data_rest = dataset.skip(train_size)\n",
        "data_valid = data_rest.take(val_size)\n",
        "data_test = data_rest.skip(val_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "cDpTcdpjA4AZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the sizes of each split\n",
        "print(f\"Total images: {total_size}\")\n",
        "print(f\"Train size: {len(data_train)}\")\n",
        "print(f\"Validation size: {len(data_valid)}\")\n",
        "print(f\"Test size: {len(data_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2z4rOGvA7lQ",
        "outputId": "3e03019d-ae99-4545-9fb3-84b4c99b9cd0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 6862\n",
            "Train size: 4803\n",
            "Validation size: 1029\n",
            "Test size: 1030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Re-laoding dataset with batch size (32)\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=abspath_curr + '/dataset/',\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,  # Use batch size of 32 for training\n",
        "    label_mode='int',  # Labels are returned as integers\n",
        "    shuffle=True  # Shuffle the dataset\n",
        ")\n",
        "print(\"\\nThe length of the dataset is \", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "189vOvHhEMG5",
        "outputId": "13b704ae-375a-4df0-d6f7-c782d6bab41d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6862 files belonging to 11 classes.\n",
            "\n",
            "The length of the dataset is  215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3zPpOR4fZjA"
      },
      "source": [
        "## Resizing the data for pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "16kuv2xPhvVU"
      },
      "outputs": [],
      "source": [
        "# Set the default input size for the pretrained model\n",
        "global input_size\n",
        "input_size = [300, 300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "oSEHIJaBC3FC"
      },
      "outputs": [],
      "source": [
        "def resize(image, label):\n",
        "    image = tf.image.resize(image, input_size)  # Resize image to input size\n",
        "    return image, label\n",
        "\n",
        "# Resize the training data\n",
        "data_train = data_train.map(resize)\n",
        "\n",
        "# Resize the validation data\n",
        "data_valid = data_valid.map(resize)\n",
        "\n",
        "# Resize the test data\n",
        "data_test = data_test.map(resize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU-V2HPPcz3P"
      },
      "source": [
        "## Preprocessing the data using pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "L0_iVsLqdbV3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "c598d2df-be6b-4181-f7f3-74c9251a8659"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'preprocess_input'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-55bfef364778>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEfficientNetB3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpreprocess_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNetB3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'preprocess_input'"
          ]
        }
      ],
      "source": [
        "# Set the preprocess_input of the pretrained model\n",
        "\n",
        "# Importing EfficientNetB3 and preprocess_input\n",
        "from tensorflow.keras.applications import EfficientNetB3\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "#preprocess_input = EfficientNetB3.preprocess_input\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0bbTGosufVb"
      },
      "outputs": [],
      "source": [
        "# Preprocess the training data using pretrained model\n",
        "data_train = data_train.map(preprocess_pretrain)\n",
        "\n",
        "# Preprocess the validation data using pretrained model\n",
        "data_valid = data_valid.map(preprocess_pretrain)\n",
        "\n",
        "# Preprocess the test data using pretrained model\n",
        "data_test = data_test.map(preprocess_pretrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBWxnpc8z6Uf"
      },
      "source": [
        "## Shuffling, batching and prefetching the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SddkzxMfz8Az"
      },
      "outputs": [],
      "source": [
        "# Shuffling the training data\n",
        "data_train = data_train.shuffle(buffer_size=1000, seed=random_seed)\n",
        "\n",
        "# Set the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Batch and prefetch the training data\n",
        "data_train = data_train.batch(batch_size).prefetch(1)\n",
        "\n",
        "# Batch and prefetch the validation data\n",
        "data_valid = data_valid.batch(batch_size).prefetch(1)\n",
        "\n",
        "# Batch and prefetch the test data\n",
        "data_test = data_test.batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMLlv9xK4AO9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l3DrD9e8g5_"
      },
      "source": [
        "## Creating the directory for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIxF9lEh8p7y"
      },
      "outputs": [],
      "source": [
        "# Make directory\n",
        "directory = os.path.dirname(abspath_curr + '/result/model/')\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIWW5z5WUQa8"
      },
      "source": [
        "## Building the architecture of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl1jL5exfwz9"
      },
      "outputs": [],
      "source": [
        "# Add the pretrained layers\n",
        "pretrained_model = keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
        "\n",
        "# Add GlobalAveragePooling2D layer\n",
        "average_pooling = keras.layers.GlobalAveragePooling2D()(pretrained_model.output)\n",
        "\n",
        "#update:\n",
        "\n",
        "dropout = keras.layers.Dropout(0.5)(average_pooling)  # Dropout to prevent overfitting\n",
        "\n",
        "# Add the output layer\n",
        "\n",
        "# output = keras.layers.Dense(n_classes, activation='softmax')(average_pooling)\n",
        "output = keras.layers.Dense(n_classes, activation='softmax')(dropout)\n",
        "\n",
        "# Get the model\n",
        "model = keras.Model(inputs=pretrained_model.input, outputs=output)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz_VacxGUTD3"
      },
      "source": [
        "## Freezing the pretrained layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEMUBOTmUXfu"
      },
      "outputs": [],
      "source": [
        "# For each layer in the pretrained model\n",
        "for layer in pretrained_model.layers:\n",
        "    # Freeze the layer\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9KTlbBT8EXs"
      },
      "source": [
        "## Setting Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_iUDJLP8K44"
      },
      "outputs": [],
      "source": [
        "# ModelCheckpoint callback\n",
        "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath=abspath_curr + '/result/model/model.weights.h5',\n",
        "                                                      save_best_only=True,\n",
        "                                                      save_weights_only=True)\n",
        "\n",
        "# EarlyStopping callback\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=2,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "# ReduceLROnPlateau callback\n",
        "reduce_lr_on_plateau_cb = keras.callbacks.ReduceLROnPlateau(factor=0.1,\n",
        "                                                            patience=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyXyvsiQVXph"
      },
      "source": [
        "## Compiling the model\n",
        "I used the default learning rate of Adam optimizer but the accuracy was less. So I changed the learning rate to 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0zAElDAVK3H"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLdyw9n_VzUb"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lES5CBZV176"
      },
      "outputs": [],
      "source": [
        "# Train, evaluate and save the best model\n",
        "history = model.fit(data_train,\n",
        "                    epochs=20,\n",
        "                    validation_data=data_valid,\n",
        "                    callbacks=[model_checkpoint_cb,\n",
        "                               early_stopping_cb,\n",
        "                               reduce_lr_on_plateau_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naOELFqc6pSB"
      },
      "source": [
        "## Plotting the learning curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b-1nRnIgL0d"
      },
      "source": [
        "### Creating the directory for the figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crm5bRITgT8Y"
      },
      "outputs": [],
      "source": [
        "# Make directory\n",
        "directory = os.path.dirname(abspath_curr + '/result/figure/')\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZucB_4TQ4APW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a figure\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "\n",
        "# Set grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Save and show the figure\n",
        "plt.tight_layout()\n",
        "plt.savefig(abspath_curr + '/result/figure/learning_curve_before_unfreezing.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey_jrLDl0bn4"
      },
      "source": [
        "## Unfreezing the pretrained layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFgtwWQ50aky"
      },
      "outputs": [],
      "source": [
        "# For each layer in the pretrained model\n",
        "for layer in pretrained_model.layers:\n",
        "    # Unfreeze the layer\n",
        "    layer.trainable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaNpWBGD2em3"
      },
      "source": [
        "## Compiling the model\n",
        "Here we use a lower learning rate (by a factor of 10) of Adam optimizer, so that it is less likely to compromise the pretrained weights.\n",
        "\n",
        "I adjusted it from 0.001 to 0.0001 because of the low test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPiXJrdW21o5"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "#               loss='sparse_categorical_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jF7fb1_3ypR"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV6dVNO532HU"
      },
      "outputs": [],
      "source": [
        "# Train, evaluate and save the best model\n",
        "history = model.fit(data_train,\n",
        "                    epochs=15,\n",
        "                    validation_data=data_valid,\n",
        "                    callbacks=[model_checkpoint_cb,\n",
        "                               early_stopping_cb,\n",
        "                               reduce_lr_on_plateau_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jooIV3Tr38rw"
      },
      "source": [
        "## Plotting the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdftIsbI3_nV"
      },
      "outputs": [],
      "source": [
        "# Create a figure\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "\n",
        "# Set grid\n",
        "plt.grid(True)\n",
        "\n",
        "# Save and show the figure\n",
        "plt.tight_layout()\n",
        "plt.savefig(abspath_curr + '/result/figure/learning_curve_after_unfreezing.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVJ3KwUub-j"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkGivY_MA6m1"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjg97rxBvTsD"
      },
      "source": [
        "## Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWOYFg8BvYI9"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "model.load_weights(filepath=abspath_curr + '/result/model/model.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAfOtSI7wajL"
      },
      "source": [
        "## Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoJnAfYfYDjH",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhmIPPeOFZ1B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}